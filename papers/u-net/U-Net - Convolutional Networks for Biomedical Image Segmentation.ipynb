{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a20c2e2",
   "metadata": {},
   "source": [
    "# U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
    " 📎 [Link to Paper](https://arxiv.org/abs/1505.04597)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dce962e5",
   "metadata": {},
   "source": [
    "## ✍️ Authors\n",
    "\n",
    "Olaf Ronneberger, Philipp Fischer, and Thomas Brox"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83008c16",
   "metadata": {},
   "source": [
    "## Difficulty\n",
    "⚫⚫⚪⚪⚪"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec1df4e2",
   "metadata": {},
   "source": [
    "## 📝 Abstract"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d1cbc4c",
   "metadata": {},
   "source": [
    "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1511883",
   "metadata": {},
   "source": [
    " ## 🏛️ Network Architecture\n",
    " ![U-Net Network Architecture](img/architecture.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0c02679",
   "metadata": {},
   "source": [
    "## 🔥 PyTorch Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8119df7f",
   "metadata": {},
   "source": [
    "### 📦 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d560a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "\n",
    "from torchvision.transforms import CenterCrop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ce3d224",
   "metadata": {},
   "source": [
    "> The network architecture is illustrated in Figure 1. It consists of a contracting path (left side) and and expansive path (right side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13709ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, contracting_path: nn.Module, expansive_path: nn.Module):\n",
    "        super().__init__()\n",
    "        self.contracting_path = contracting_path\n",
    "        self.expansive_path = expansive_path\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, feature_maps = self.contracting_path(x)\n",
    "        return self.expansive_path(x, feature_maps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2a61506",
   "metadata": {},
   "source": [
    "### Contractive Path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0bb56033",
   "metadata": {},
   "source": [
    "Note that the contracting path (left side) has two outputs `x` and `feature maps`. `x` is the output of the last block of the `contracting_path` and `feature_maps` are the 4 feature maps that are needed for the expansive_path (gray arrows)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88f130f1",
   "metadata": {},
   "source": [
    "> The contracting path follows the typical architecture of a convolutional network. It consists of the repeated application of two 3x3 convolutions (unpadded convolutions), [...] At each downsampling step we double the number of feature channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ac0b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContractivePath(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.pooling = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv_blocks = nn.ModuleList([\n",
    "            ConvBlock(in_channels=in_channels, out_channels=64),\n",
    "            ConvBlock(in_channels=64, out_channels=128),\n",
    "            ConvBlock(in_channels=128, out_channels=256),\n",
    "            ConvBlock(in_channels=256, out_channels=512),\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x) -> list:\n",
    "        feature_maps = []\n",
    "        for i, block in enumerate(self.conv_blocks):\n",
    "            x = block(x)\n",
    "            feature_maps.append(x)\n",
    "            x = self.pooling(x)\n",
    "        \n",
    "        return x, feature_maps[::-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e1077eb",
   "metadata": {},
   "source": [
    "> ... each followed by a rectified linear unit (ReLU) and a 2x2 max pooling operation with stride 2 for downsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6868c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int , kernel_size: int = 3, stride: int = 1, padding: int = 0):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            self.conv1,\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            self.conv2,\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.conv_block(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13caac2b",
   "metadata": {},
   "source": [
    "### Expansive Path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3822997",
   "metadata": {},
   "source": [
    "> Every step in the expansive path consists of an upsampling of the feature map followed by a 2x2 convolution (“up-convolution”) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each followed by a ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028cc5c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ExpansivePath(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.conv_blocks = nn.ModuleList([\n",
    "            ConvBlock(in_channels=512, out_channels=1024),\n",
    "            ConvBlock(in_channels=1024, out_channels=512),\n",
    "            ConvBlock(in_channels=512, out_channels=256),\n",
    "            ConvBlock(in_channels=256, out_channels=128),\n",
    "            ConvBlock(in_channels=128, out_channels=64),\n",
    "        ])\n",
    "\n",
    "        self.up_convs = nn.ModuleList([\n",
    "            nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=2, stride=2),\n",
    "            nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=2, stride=2),\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=2, stride=2),\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=64, out_channels=n_classes, kernel_size=1, stride=1)\n",
    "        ])\n",
    "\n",
    "            \n",
    "    def forward(self, x, feature_maps):\n",
    "        \n",
    "        x = self.conv_blocks[0](x)\n",
    "        x = self.up_convs[0](x)\n",
    "\n",
    "        for block, up_conv, feature_map in zip(self.conv_blocks[1:], self.up_convs[1:], feature_maps):\n",
    "            size = feature_map.size()[-2:] # [batch size ,channels, height, width]\n",
    "            # crop feature map to match the size of x\n",
    "            #feature_map = CenterCrop(size)(feature_map)\n",
    "            x = nn.functional.interpolate(x, size=size, mode='bilinear', align_corners=True)\n",
    "            # concatenate feature map and x\n",
    "            x = torch.cat((feature_map, x), dim=1)\n",
    "            # pass through conv block and up conv\n",
    "            x = block(x)\n",
    "            x = up_conv(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6c6c697",
   "metadata": {},
   "source": [
    "> At the final layer a 1x1 convolution is used to map each 64component feature vector to the desired number of classes. In total the network has 23 convolutional layers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48e72f3b",
   "metadata": {},
   "source": [
    "## 🏗️ Creating the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beae3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates a model for U-Net architecture\n",
    "# It takes the number of input channels and the number of classes as parameters\n",
    "# It returns an instance of the U-Net model\n",
    "def make_model(in_channels: int, n_classes: int) -> nn.Module:\n",
    "    contractive_path = ContractivePath(in_channels = in_channels)\n",
    "    expansive_path = ExpansivePath(n_classes = n_classes)\n",
    "    u_net = UNet(contractive_path, expansive_path)\n",
    "    return u_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c0fde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the U-Net model\n",
    "u_net = make_model(in_channels = 3, n_classes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a813ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate pseudo batch of images\n",
    "img = torch.randn(8, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f1339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_net(img).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "752d9c3e",
   "metadata": {},
   "source": [
    "## 💥 U-Net in Action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b5f1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import VOCSegmentation\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef6625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VOCSegmentation(\n",
    "    root='data',\n",
    "    year='2012',\n",
    "    image_set='train',\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((256, 256)),\n",
    "        \n",
    "        ]\n",
    "    ),\n",
    "    target_transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((248, 248)),\n",
    "        # 1 if pixel is > 0 else 0\n",
    "        transforms.Lambda(lambda x: torch.where(x > 0, torch.ones_like(x), torch.zeros_like(x)))\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a57ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dl = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d64c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, dl, loss_fn, optimizer, device, epochs):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch+1}/{epochs}')\n",
    "        pbar = tqdm(dl)\n",
    "        for i, batch in enumerate(pbar):\n",
    "            # move batch to device\n",
    "            batch = [b.to(device) for b in batch]\n",
    "            # get images and labels\n",
    "            images, labels = batch\n",
    "            # zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            images = images.to(device)\n",
    "            preds = model(images)#.view(8, -1)\n",
    "            #labels = labels.view(8, -1)\n",
    "            # compute loss\n",
    "            loss = loss_fn(preds, labels)\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "            # print loss\n",
    "            pbar.set_description(f'Loss: {loss.item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2878f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(u_net, dl, nn.BCEWithLogitsLoss(), torch.optim.Adam(u_net.parameters(), lr=0.001), 'cuda', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507e4434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor to image\n",
    "def tensor_to_image(tensor):\n",
    "    image = transforms.ToPILImage()(tensor)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34d1c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cdab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = batch[0][0]\n",
    "mask = batch[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d531426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_to_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65fc564",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_to_image(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44120aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = u_net(img.to('cuda').unsqueeze(0)).sigmoid()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d654db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_to_image(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c486edf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_to_image(torch.where(pred > 0.5, torch.ones_like(pred), torch.zeros_like(pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff5a7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to show on image as an overlay\n",
    "def show_overlay(img, mask, alpha=0.5):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    # import cv2\n",
    "    import cv2\n",
    "    # convert to numpy\n",
    "    img = img.detach().cpu().numpy()\n",
    "    mask = mask.detach().cpu().numpy()\n",
    "    # transpose to (height, width, channels)\n",
    "    img = img.transpose(1, 2, 0)\n",
    "    # transpose mask to (height, width, channels)\n",
    "    mask = mask.transpose(1, 2, 0)\n",
    "    # concat to additional channels of all zeros\n",
    "    mask = np.concatenate([mask, np.zeros_like(mask), np.zeros_like(mask)], axis=-1)\n",
    "    # resize mask\n",
    "    mask = cv2.resize(mask, (img.shape[1], img.shape[0]))\n",
    "    # create a figure\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    # show image\n",
    "    ax.imshow(img)    \n",
    "    ax.imshow(mask, alpha=alpha)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "86c4a56b",
   "metadata": {},
   "source": [
    "##  📚 References"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ba2d5a9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "cite2c": {
   "citations": {
    "8790778/JYJPV3P2": {
     "DOI": "10.48550/arXiv.1603.07285",
     "URL": "http://arxiv.org/abs/1603.07285",
     "abstract": "We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.",
     "accessed": {
      "date-parts": [
       [
        2023,
        1,
        13
       ]
      ]
     },
     "author": [
      {
       "family": "Dumoulin",
       "given": "Vincent"
      },
      {
       "family": "Visin",
       "given": "Francesco"
      }
     ],
     "issued": {
      "date-parts": [
       [
        2018,
        1,
        11
       ]
      ]
     },
     "note": "arXiv:1603.07285 [cs, stat]",
     "publisher": "arXiv",
     "title": "A guide to convolution arithmetic for deep learning",
     "type": "article"
    }
   }
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 13:09:58) \n[GCC 7.5.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "9eed5966493f68e027e6b1ab9ca8fa21f01ff6bffd0d335eb465768cd896c1f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
